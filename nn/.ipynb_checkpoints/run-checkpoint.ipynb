{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python two_layer_net_nn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!open ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python torch_tmp12.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "w = 2\n",
    "b = 1\n",
    "noise = torch.rand(100, 1)\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)\n",
    "# 因为输入层格式要为(-1, 1)，所以这里将(100)的格式转成(100, 1)\n",
    "y = w*x + b + noise\n",
    "# 拟合分布在y=2x+1上并且带有噪声的散点\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 16),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(16, 1),\n",
    "    )\n",
    "# 自定义的网络，带有2个全连接层和一个tanh层\n",
    "loss_fun = torch.nn.MSELoss()\n",
    "# 定义损失函数为均方差\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# 使用adam作为优化器更新网络模型的权重，学习率为0.01\n",
    "\n",
    "plt.ion()\n",
    "# 图形交互\n",
    "for _ in range(10000):\n",
    "    ax = plt.axes()\n",
    "    output = model(x)\n",
    "    # 数据向后传播（经过网络层的一次计算）\n",
    "    loss = loss_fun(output, y)\n",
    "    # 计算损失值\n",
    "    # print(\"before zero_grad:{}\".format(list(model.children())[0].weight.grad))\n",
    "    # print(\"-\"*100)\n",
    "    model.zero_grad()\n",
    "    # 优化器清空梯度\n",
    "    # print(\"before zero_grad:{}\".format(list(model.children())[0].weight.grad))\n",
    "    # print(\"-\"*100)\n",
    "    # 通过注释地方可以对比发现执行zero_grad方法以后倒数梯度将会被清0\n",
    "    # 如果不清空梯度的话，则会不断累加梯度，从而影响到当前梯度的计算\n",
    "    loss.backward()\n",
    "    # 向后传播，计算当前梯度，如果这步不执行，那么优化器更新时则会找不到梯度\n",
    "    optimizer.step()\n",
    "    # 优化器更新梯度参数，如果这步不执行，那么因为梯度没有发生改变，loss会一直计算最开始的那个梯度\n",
    "    if _ % 100 == 0:\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), output.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        # print(\"w:\", list(model.children())[0].weight.t() @ list(model.children())[-1].weight.t())\n",
    "        # 通过这句可以查看权值变化，可以发现最后收敛到2附近\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
